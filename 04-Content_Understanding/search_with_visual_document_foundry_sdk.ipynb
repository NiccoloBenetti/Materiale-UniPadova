{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Visual Document Search with Azure Content Understanding and AI Foundry SDK\n",
        "\n",
        "Source: https://github.com/Azure-Samples/azure-ai-search-with-content-understanding-python\n",
        "\n",
        "(modified for training purposes)\n",
        "\n",
        "## Objective\n",
        "This document illustrates an example workflow for how to leverage the Azure AI Content Understanding API to enhance the quality of document search.\n",
        "\n",
        "The sample will demonstrate the following steps:\n",
        "1. Extract the layout and content of a document using Azure AI Document Intelligence.\n",
        "2. For each figure in the document, extract its content with a custom analyzer using Azure AI Content Understanding, and insert it into the corresponding location in the document content.\n",
        "2. Chunk and embed the document content with LangChain and Azure OpenAI, and index them with Azure Search to generate an Azure Search index.\n",
        "3. Utilize an OpenAI chat model to search through content in the document with a natural language query.\n",
        "\n",
        "\n",
        "## Pre-requisites\n",
        "1. Follow the [README](README.md) to create the required resources for this sample.\n",
        "1. Install the required packages.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load environment variables"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from datetime import datetime # added for customizing AZURE_SEARCH_INDEX_NAME (if needed)\n",
        "\n",
        "load_dotenv(dotenv_path='../infra/credentials.env', override=True)\n",
        "\n",
        "# Load and validate Azure AI Services configs\n",
        "AZURE_AI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_SERVICE_ENDPOINT\")\n",
        "AZURE_AI_SERVICE_API_VERSION = os.getenv(\"AZURE_AI_SERVICE_API_VERSION\") or \"2024-12-01-preview\"\n",
        "AZURE_DOCUMENT_INTELLIGENCE_API_VERSION = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_API_VERSION\") or \"2024-11-30\"\n",
        "\n",
        "# Load and validate Azure OpenAI configs\n",
        "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"MODEL_DEPLOYMENT_NAME\")\n",
        "AZURE_OPENAI_CHAT_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\") or \"2024-08-01-preview\"\n",
        "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"EMBEDDING_DEPLOYMENT_NAME\")\n",
        "AZURE_OPENAI_EMBEDDING_API_VERSION = os.getenv(\"AZURE_OPENAI_EMBEDDING_API_VERSION\") or \"2023-05-15\"\n",
        "\n",
        "# Load and validate Azure Search Services configs\n",
        "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
        "AZURE_SEARCH_KEY = os.getenv(\"AZURE_SEARCH_KEY\")\n",
        "AZURE_SEARCH_INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") or \"sample-index-visual-doc\""
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1744303342808
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Current Azure AI Services endpoint: {AZURE_AI_SERVICE_ENDPOINT}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Current Azure AI Services endpoint: https://ai-aifoundryupskillinghub687267079310.cognitiveservices.azure.com/\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1744303343069
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Current Azure OpenAI endpoint: {AZURE_OPENAI_ENDPOINT}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Current Azure OpenAI endpoint: https://ai-aifoundryupskillinghub687267079310.openai.azure.com/\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1744303343472
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Current Azure AI Search endpoint: {AZURE_SEARCH_ENDPOINT}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Current Azure AI Search endpoint: https://ai-search-abutneva687267079310.search.windows.net\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1744303343895
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File to analyze"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the path to the file that will be analyzed\n",
        "# Sample report source: https://www.imf.org/en/Publications/CR/Issues/2024/07/18/United-States-2024-Article-IV-Consultation-Press-Release-Staff-Report-and-Statement-by-the-552100\n",
        "file = Path(\"assets/reports/sample_report_3pg.pdf\")"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1744303344259
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "PosixPath('assets/reports/sample_report_3pg.pdf')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1744303344941
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create custom analyzer using chart and diagram understanding template"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import sys\n",
        "import uuid\n",
        "import pandas as pd # added for visualizing existing analyzers into a df\n",
        "import logging # added for visualizing response details from methods e.g. delete_analyzer()"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1744303345298
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# only if necessary, add the parent directory to the path to use shared modules\n",
        "# parent_dir = Path(Path.cwd()).parent\n",
        "# sys.path.append(str(parent_dir))\n",
        "\n",
        "# import the utility class AzureContentUnderstandingClient, which is a wrapper around the Azure Content Understanding REST API client\n",
        "from python.content_understanding_client import AzureContentUnderstandingClient"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1744303345742
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
        "credential = DefaultAzureCredential()\n",
        "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1744303345943
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try:\n",
        "#     credential = DefaultAzureCredential()\n",
        "#     # Test token acquisition\n",
        "#     token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
        "#     print(\"Successfully acquired token!\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Authentication failed: {str(e)}\")"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1744303346250
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create content understanding client"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "content_understanding_client = AzureContentUnderstandingClient(\n",
        "    endpoint=AZURE_AI_SERVICE_ENDPOINT,\n",
        "    api_version=AZURE_AI_SERVICE_API_VERSION,\n",
        "    # subscription_key= \"715b91dd-7c91-4bf2-8987-8640c7168071\",\n",
        "    token_provider=token_provider,\n",
        "    # x_ms_useragent=\"azure-ai-content-understanding-python/search_with_visusal_document\", # This header is used for sample usage telemetry, please comment out this line if you want to opt out.\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1744303346563
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create an analyzer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get path to sample template\n",
        "ANALYZER_TEMPLATE_PATH = \"analyzer_templates/image_chart_diagram_understanding.json\""
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1744303346846
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create analyzer\n",
        "ANALYZER_ID = \"content-understanding-search-sample-\" + str(uuid.uuid4())\n",
        "print(f\"Creating analyzer with ID '{ANALYZER_ID}'...\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Creating analyzer with ID 'content-understanding-search-sample-4d23a4ef-c9f3-4225-91f7-b935ab815914'...\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1744303347258
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    response = content_understanding_client.begin_create_analyzer(ANALYZER_ID, analyzer_template_path=ANALYZER_TEMPLATE_PATH)\n",
        "    result = content_understanding_client.poll_result(response)\n",
        "    print(f'Analyzer details for {result[\"result\"][\"analyzerId\"]}:')\n",
        "    # print(json.dumps(result, indent=2))\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    print(\"Error in creating analyzer. Please double-check your analysis settings.\\nIf there is a conflict, you can delete the analyzer and then recreate it, or move to the next cell and use the existing analyzer.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Analyzer details for content-understanding-search-sample-4d23a4ef-c9f3-4225-91f7-b935ab815914:\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1744303347629
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the analyzer was created successfully\n",
        "result = content_understanding_client.get_analyzer_detail_by_id(ANALYZER_ID)\n",
        "print(json.dumps(result, indent=2))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{\n  \"analyzerId\": \"content-understanding-search-sample-4d23a4ef-c9f3-4225-91f7-b935ab815914\",\n  \"description\": \"Extract detailed structured information from charts and diagrams.\",\n  \"createdAt\": \"2025-04-10T16:42:26Z\",\n  \"lastModifiedAt\": \"2025-04-10T16:42:26Z\",\n  \"config\": {\n    \"returnDetails\": false,\n    \"disableContentFiltering\": false\n  },\n  \"fieldSchema\": {\n    \"name\": \"ChartsAndDiagrams\",\n    \"fields\": {\n      \"Title\": {\n        \"type\": \"string\",\n        \"description\": \"Verbatim title of the chart.\"\n      },\n      \"ChartType\": {\n        \"type\": \"string\",\n        \"description\": \"The type of chart.\",\n        \"enum\": [\n          \"area\",\n          \"bar\",\n          \"box\",\n          \"bubble\",\n          \"candlestick\",\n          \"funnel\",\n          \"heatmap\",\n          \"histogram\",\n          \"line\",\n          \"pie\",\n          \"radar\",\n          \"rings\",\n          \"rose\",\n          \"treemap\"\n        ],\n        \"enumDescriptions\": {\n          \"histogram\": \"Continuous values on the x-axis, which distinguishes it from bar.\",\n          \"rose\": \"In contrast to pie charts, the sectors are of equal angles and differ in how far each sector extends from the center of the circle.\"\n        }\n      },\n      \"TopicKeywords\": {\n        \"type\": \"array\",\n        \"description\": \"Relevant topics associated with the chart, used for tagging.\",\n        \"items\": {\n          \"type\": \"string\",\n          \"examples\": [\n            \"Business and finance\",\n            \"Arts and culture\",\n            \"Education and academics\"\n          ]\n        }\n      },\n      \"DetailedDescription\": {\n        \"type\": \"string\",\n        \"description\": \"Detailed description of the chart or diagram, not leaving out any key information. Include numbers, trends, and other details.\"\n      },\n      \"Summary\": {\n        \"type\": \"string\",\n        \"description\": \"Detailed summary of the chart, including highlights and takeaways.\"\n      },\n      \"MarkdownDataTable\": {\n        \"type\": \"string\",\n        \"description\": \"Underlying data of the chart in tabular markdown format. Give markdown output with valid syntax and accurate numbers, and fill any uncertain values with empty cells. If not applicable, output an empty string.\"\n      },\n      \"AxisTitles\": {\n        \"type\": \"object\",\n        \"description\": \"Titles of the x and y axes.\",\n        \"properties\": {\n          \"xAxisTitle\": {\n            \"type\": \"string\"\n          },\n          \"yAxisTitle\": {\n            \"type\": \"string\"\n          }\n        }\n      },\n      \"FootnotesAndAnnotations\": {\n        \"type\": \"string\",\n        \"description\": \"All footnotes and textual annotations in the chart or diagram.\"\n      }\n    }\n  },\n  \"warnings\": [],\n  \"status\": \"ready\",\n  \"scenario\": \"image\"\n}\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1744303347839
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze document layout and compose with figure descriptions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions for document-figure composition"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install PyMuPDF\n",
        "\n",
        "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
        "from azure.ai.documentintelligence.models import AnalyzeResult\n",
        "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n",
        "import fitz\n",
        "from PIL import Image"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1744303354776
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define helper functions for document-figure composition\n",
        "def insert_figure_contents(md_content, figure_contents, span_offsets):\n",
        "    \"\"\"\n",
        "    Inserts the figure content for each of the provided figures in figure_contents\n",
        "    before the span offset of that figure in the given markdown content.\n",
        "\n",
        "    Args:\n",
        "    - md_content (str): The original markdown content.\n",
        "    - figure_contents (list[str]): The contents of each figure to insert.\n",
        "    - span_offsets (list[int]): The span offsets of each figure in order. These should be sorted and strictly increasing.\n",
        "\n",
        "    Returns:\n",
        "    - str: The modified markdown content with the the figure contents prepended to each figure's span.\n",
        "    \"\"\"\n",
        "    # NOTE: In this notebook, we only alter the Markdown content returned by the Document Intelligence API,\n",
        "    # and not the per-element spans in the API response. Thus, after figure content insertion, these per-element spans will be inaccurate.\n",
        "    # This may impact use cases like citation page number calculation.\n",
        "    # Additional code may be needed to correct the spans or otherwise infer the page numbers for each citation.\n",
        "    # The main purpose of the notebook is to show the feasibility of using Content Understanding with Azure Search for RAG chat applications.\n",
        "\n",
        "    # Validate span_offsets are sorted and strictly increasing\n",
        "    if span_offsets != sorted(span_offsets) or not all([o < span_offsets[i + 1] for i, o in enumerate(span_offsets) if i < len(span_offsets) - 1]):\n",
        "        raise ValueError(\"span_offsets should be sorted and strictly increasing.\")\n",
        "\n",
        "    # Split the content based on the provided spans\n",
        "    parts = []\n",
        "    preamble = None\n",
        "    for i, offset in enumerate(span_offsets):\n",
        "        if i == 0 and offset > 0:\n",
        "            preamble = md_content[0:offset]\n",
        "            parts.append(md_content[offset:span_offsets[i + 1]])\n",
        "        elif i == len(span_offsets) - 1:\n",
        "            parts.append(md_content[offset:])\n",
        "        else:\n",
        "            parts.append(md_content[offset:span_offsets[i + 1]])\n",
        "\n",
        "    # Join the parts back together with the figure content inserted\n",
        "    modified_content = \"\"\n",
        "    if preamble:\n",
        "        modified_content += preamble\n",
        "    for i, part in enumerate(parts):\n",
        "        modified_content += f\"<!-- FigureContent=\\\"{figure_contents[i]}\\\" -->\" + part\n",
        "\n",
        "    return modified_content\n",
        "\n",
        "def crop_image_from_pdf_page(pdf_path, page_number, bounding_box):\n",
        "    \"\"\"\n",
        "    Crops a region from a given page in a PDF and returns it as an image.\n",
        "\n",
        "    Args:    \n",
        "    - pdf_path (pathlib.Path): Path to the PDF file.\n",
        "    - page_number (int): The page number to crop from (0-indexed).\n",
        "    - bounding_box (tuple): A tuple of (x0, y0, x1, y1) coordinates for the bounding box.\n",
        "    \n",
        "    Returns:\n",
        "    - PIL.Image: A PIL Image of the cropped area.\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    page = doc.load_page(page_number)\n",
        "    \n",
        "    # Cropping the page. The rect requires the coordinates in the format (x0, y0, x1, y1).\n",
        "    bbx = [x * 72 for x in bounding_box]\n",
        "    rect = fitz.Rect(bbx)\n",
        "    pix = page.get_pixmap(matrix=fitz.Matrix(300 / 72, 300 / 72), clip=rect)\n",
        "    \n",
        "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "    \n",
        "    doc.close()\n",
        "\n",
        "    return img\n",
        "\n",
        "def format_content_understanding_result(content_understanding_result):\n",
        "    \"\"\"\n",
        "    Formats the JSON output of the Content Understanding result as Markdown for downstream usage in text.\n",
        "    \n",
        "    Args:\n",
        "    - content_understanding_result (dict): A dictionary containing the output from Content Understanding.\n",
        "\n",
        "    Returns:\n",
        "    - str: A Markdown string of the result content.\n",
        "    \"\"\"\n",
        "    def _format_result(key, result):\n",
        "        result_type = result[\"type\"]\n",
        "        if result_type in [\"string\", \"integer\", \"number\", \"boolean\"]:\n",
        "            return f\"**{key}**: \" + str(result[f'value{result_type.capitalize()}']) + \"\\n\"\n",
        "        elif result_type == \"array\":\n",
        "            return f\"**{key}**: \" + ', '.join([str(result[\"valueArray\"][i][f\"value{r['type'].capitalize()}\"]) for i, r in enumerate(result[\"valueArray\"])]) + \"\\n\"\n",
        "        elif result_type == \"object\":\n",
        "            return f\"**{key}**\\n\" + ''.join([_format_result(f\"{key}.{k}\", result[\"valueObject\"][k]) for k in result[\"valueObject\"]])\n",
        "\n",
        "    fields = content_understanding_result['result']['contents'][0]['fields']\n",
        "    markdown_result = \"\"\n",
        "    for field in fields:\n",
        "        markdown_result += _format_result(field, fields[field])\n",
        "\n",
        "    return markdown_result"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1744303355767
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract figures and run content understanding"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import json\n",
        "import os"
      ],
      "outputs": [],
      "execution_count": 90,
      "metadata": {
        "gather": {
          "logged": 1744303066525
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "PosixPath('assets/reports/sample_report_3pg.pdf')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1744303359751
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# observed computation time: ~1 minute for 3 pages\n",
        "# the output is cached in a file called 'sample_report.cache'"
      ],
      "outputs": [],
      "execution_count": 62,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Content Understanding on each figure, format figure contents, and insert figure contents into corresponding document locations\n",
        "with open(file, 'rb') as f:\n",
        "    pdf_bytes = f.read()\n",
        "\n",
        "    document_intelligence_client = DocumentIntelligenceClient(\n",
        "        endpoint=AZURE_AI_SERVICE_ENDPOINT,\n",
        "        api_version=AZURE_DOCUMENT_INTELLIGENCE_API_VERSION,\n",
        "        credential=credential,\n",
        "        output=str('figures')\n",
        "    )\n",
        "\n",
        "    poller = document_intelligence_client.begin_analyze_document(\n",
        "        \"prebuilt-layout\",\n",
        "        AnalyzeDocumentRequest(bytes_source=pdf_bytes),\n",
        "        output=[str('figures')],\n",
        "        features=['ocrHighResolution'],\n",
        "        output_content_format=\"markdown\"\n",
        "    )\n",
        "\n",
        "    result: AnalyzeResult = poller.result()\n",
        "    \n",
        "    md_content = result.content\n",
        "\n",
        "    figure_contents = []\n",
        "    if result.figures:\n",
        "        print(\"Extracting figure contents with Content Understanding.\")\n",
        "        for figure_idx, figure in enumerate(result.figures):\n",
        "            for region in figure.bounding_regions:\n",
        "                    # Uncomment the below to print out the bounding regions of each figure\n",
        "                    # print(f\"Figure {figure_idx + 1} body bounding regions: {region}\")\n",
        "                    # To learn more about bounding regions, see https://aka.ms/bounding-region\n",
        "                    bounding_box = (\n",
        "                            region.polygon[0],  # x0 (left)\n",
        "                            region.polygon[1],  # y0 (top\n",
        "                            region.polygon[4],  # x1 (right)\n",
        "                            region.polygon[5]   # y1 (bottom)\n",
        "                        )\n",
        "            page_number = figure.bounding_regions[0]['pageNumber']\n",
        "            cropped_img = crop_image_from_pdf_page(file, page_number - 1, bounding_box)\n",
        "\n",
        "            os.makedirs(\"figures\", exist_ok=True)\n",
        "\n",
        "            figure_filename = f\"figure_{figure_idx + 1}.png\"\n",
        "            # Full path for the file\n",
        "            figure_filepath = os.path.join(\"figures\", figure_filename)\n",
        "\n",
        "            # Save the figure\n",
        "            cropped_img.save(figure_filepath)\n",
        "            bytes_io = io.BytesIO()\n",
        "            cropped_img.save(bytes_io, format='PNG')\n",
        "            cropped_img = bytes_io.getvalue()\n",
        "\n",
        "            # Collect formatted content from the figure\n",
        "            content_understanding_response = content_understanding_client.begin_analyze(ANALYZER_ID, figure_filepath)\n",
        "            content_understanding_result = content_understanding_client.poll_result(content_understanding_response, timeout_seconds=1000)\n",
        "            figure_content = format_content_understanding_result(content_understanding_result)\n",
        "            figure_contents.append(figure_content)\n",
        "            print(f\"Figure {figure_idx + 1} contents:\\n{figure_content}\")\n",
        "\n",
        "        # Insert figure content into corresponding location in document\n",
        "        md_content = insert_figure_contents(md_content, figure_contents, [f.spans[0][\"offset\"] for f in result.figures])\n",
        "    \n",
        "    # Save results as a JSON file to cache the result for downstream use\n",
        "    result.content = md_content\n",
        "    output = {}\n",
        "    output['analyzeResult'] = result.as_dict()\n",
        "    output = json.dumps(output)\n",
        "    with open('sample_report.cache', 'w') as f:\n",
        "        f.write(output)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Extracting figure contents with Content Understanding.\nFigure 1 contents:\n**Title**: 2023 Real GDP\n**ChartType**: bar\n**TopicKeywords**: Business and finance, Economics\n**DetailedDescription**: The bar chart displays the percent deviation of the 2023 Real GDP from the pre-crisis trend for various regions. The United States shows a positive deviation of approximately 1.0%, indicating growth above the pre-crisis trend. Japan, Canada, and the Euro Area have negative deviations, with Japan and Canada around -1.0% and the Euro Area slightly more negative. The United Kingdom has the largest negative deviation at approximately -4.0%, indicating significant underperformance compared to the pre-crisis trend. The G-20 Emerging Markets (EMs) also show a negative deviation, slightly less than the UK, around -3.5%.\n**Summary**: In 2023, the United States is the only region with a positive GDP deviation from the pre-crisis trend, while the UK shows the largest negative deviation. Other regions like Japan, Canada, Euro Area, and G-20 EMs also have negative deviations, indicating economic underperformance compared to pre-crisis expectations.\n**MarkdownDataTable**: | Region    | Percent Deviation |\n|-----------|------------------|\n| US        | 1.0              |\n| Japan     | -1.0             |\n| Canada    | -1.0             |\n| Euro Area | -1.5             |\n| UK        | -4.0             |\n| G-20 EMs  | -3.5             |\n**AxisTitles**\n**AxisTitles.xAxisTitle**: \n**AxisTitles.yAxisTitle**: Percent Deviation\n**FootnotesAndAnnotations**: (percent deviation from pre-crisis trend)\n\nFigure 2 contents:\n**Title**: Federal Government Spending\n**ChartType**: bar\n**TopicKeywords**: Business and finance, Economics, Government\n**DetailedDescription**: The chart displays federal government spending as a percentage of GDP, relative to the 2019 level, from 2020 to 2023. In 2020, spending was approximately 9% of GDP, with the majority allocated to individuals, followed by SLGs (State and Local Governments), businesses, and others. In 2021, spending decreased to around 5% of GDP, with individuals still receiving the largest share, followed by SLGs, businesses, and others. By 2022, spending further reduced to about 1% of GDP, with individuals and SLGs receiving similar shares, and businesses and others receiving smaller portions. In 2023, spending was slightly above 0% of GDP, with individuals receiving the largest share, followed by SLGs, businesses, and others.\n**Summary**: The chart illustrates a declining trend in federal government spending as a percentage of GDP from 2020 to 2023, relative to the 2019 level. The largest share of spending consistently went to individuals, followed by SLGs, businesses, and others. Spending peaked in 2020 and gradually decreased each year.\n**MarkdownDataTable**: | Year | Individuals | Businesses | SLGs | Others |\n|------|-------------|------------|------|--------|\n| 2020 | 6           | 1          | 1    | 1      |\n| 2021 | 3           | 1          | 1    | 0      |\n| 2022 | 0.5         | 0.2        | 0.2  | 0.1    |\n| 2023 | 0.2         | 0.1        | 0.1  | 0.1    |\n**AxisTitles**\n**AxisTitles.xAxisTitle**: Year\n**AxisTitles.yAxisTitle**: Percent of GDP\n**FootnotesAndAnnotations**: Source: U.S. Department of Treasury; Small Business Administration; Congressional Research Services; Office of Management and Budget; IMF staff calculation.\nNote: 'Federal spending is measured as the increase relative to the 2019 level; Others included defense spending, capital investment, interest spending, and other items not directly attributable to firms, households, or subnational governments.\n\nFigure 3 contents:\n**Title**: General Government Structural Primary Balance\n**ChartType**: bar\n**TopicKeywords**: Business and finance, Economics, Fiscal policy\n**DetailedDescription**: The chart compares the general government structural primary balance between the US and the Euro Area for the pre-pandemic year 2019 and the fiscal impulse period. The structural primary balance for the US in 2019 was approximately -3, while for the Euro Area it was slightly better, around -2. During the fiscal impulse period, the US saw a cumulative deficit increase to about -8, whereas the Euro Area's deficit increased more significantly to around -12.\n**Summary**: The chart illustrates the change in general government structural primary balance from 2019 to the fiscal impulse period for the US and Euro Area. The Euro Area experienced a larger increase in cumulative deficits compared to the US.\n**MarkdownDataTable**: | Period                  | US Balance | Euro Area Balance |\n|-------------------------|------------|-------------------|\n| Pre-pandemic (2019)     | -3         | -2                |\n| Fiscal impulse          | -8         | -12               |\n**AxisTitles**\n**AxisTitles.xAxisTitle**: Period\n**AxisTitles.yAxisTitle**: General Government Structural Primary Balance\n**FootnotesAndAnnotations**: \n\nFigure 4 contents:\n**Title**: Asset Price Indices (Dec 2019 = 100)\n**ChartType**: line\n**TopicKeywords**: Business and finance, Economics, Real estate, Stock market\n**DetailedDescription**: The chart displays two asset price indices: the Case-Shiller House Price Index and the S&P 500 Index, both normalized to a base value of 100 in December 2019. The Case-Shiller House Price Index shows a steady increase from December 2019, reaching approximately 160 by September 2023. The S&P 500 Index exhibits more volatility, with a sharp increase in early 2020, followed by fluctuations throughout 2021 and 2022, and a significant rise in 2023, peaking at around 170.\n**Summary**: The Case-Shiller House Price Index has shown a consistent upward trend since December 2019, indicating a steady increase in house prices. In contrast, the S&P 500 Index has experienced more volatility but ultimately shows a strong upward trend, surpassing the house price index by September 2023.\n**MarkdownDataTable**: | Date     | Case-Shiller House Price Index | S&P 500 Index |\n|----------|--------------------------------|---------------|\n| Dec-2019 | 100                            | 100           |\n| Sep-2020 | 110                            | 120           |\n| Jun-2021 | 130                            | 140           |\n| Mar-2022 | 140                            | 130           |\n| Dec-2022 | 150                            | 140           |\n| Sep-2023 | 160                            | 170           |\n**AxisTitles**\n**AxisTitles.xAxisTitle**: \n**AxisTitles.yAxisTitle**: \n**FootnotesAndAnnotations**: Source: S&P Dow Jones Indices LLC; Haver.\n\nFigure 5 contents:\n**Title**: (US$ trillions, constant 2019 prices)\n**ChartType**: bar\n**TopicKeywords**: Business and finance, Economics, Income distribution\n**DetailedDescription**: The bar chart displays the distribution of income across different segments of the population, divided into quintiles, for the years 2009, 2019, and 2023. The income is measured in US$ trillions, adjusted to constant 2019 prices. The segments are Bottom 20%, 21-40%, 41-60%, 61-80%, and Top 20% (RHS). The Top 20% segment is shown on a secondary axis (RHS) with a different scale. In 2009, the Bottom 20% had the lowest income, while the Top 20% had the highest. Over the years, income has increased across all segments, with the Top 20% showing the most significant growth. By 2023, the Top 20% segment reached nearly 90 trillion, while the Bottom 20% remained below 2 trillion.\n**Summary**: The chart illustrates the growth in income across different population segments from 2009 to 2023, with the Top 20% showing the most substantial increase. The income distribution is skewed towards the Top 20%, which has consistently held the largest share of income over the years.\n**MarkdownDataTable**: | Segment   | 2009 | 2019 | 2023 |\n|-----------|------|------|------|\n| Bottom 20%| 1    | 2    | 2    |\n| 21-40%    | 2    | 3    | 3    |\n| 41-60%    | 3    | 4    | 5    |\n| 61-80%    | 5    | 7    | 8    |\n| Top 20%   | 60   | 80   | 90   |\n**AxisTitles**\n**AxisTitles.xAxisTitle**: Income Quintiles\n**AxisTitles.yAxisTitle**: Income (US$ trillions)\n**FootnotesAndAnnotations**: Source: Federal Reserve; Bureau of Economic Analysis\n\nFigure 6 contents:\n**Title**: \n**ChartType**: line\n**TopicKeywords**: Business and finance, Economics\n**DetailedDescription**: The line chart shows a trend over time from 2018 Q1 to 2024 Q1. The y-axis values range from 1.5 to 3.0. Initially, from 2018 Q1 to 2019 Q3, the values hover around 2.5. There is a noticeable decline starting in 2020 Q1, reaching a low point around 1.5 in 2021 Q1. After this, the values begin to rise sharply, surpassing 2.5 by 2022 Q3 and reaching approximately 3.0 by 2024 Q1.\n**Summary**: The chart illustrates a significant dip in values from 2020 Q1 to 2021 Q1, followed by a strong recovery and upward trend reaching 3.0 by 2024 Q1. This suggests a period of economic downturn followed by recovery.\n**MarkdownDataTable**: | Quarter | Value |\n|---------|-------|\n| 2018Q1  | 2.5   |\n| 2018Q4  | 2.5   |\n| 2019Q3  | 2.6   |\n| 2020Q2  | 2.3   |\n| 2021Q1  | 1.5   |\n| 2021Q4  | 1.7   |\n| 2022Q3  | 2.6   |\n| 2023Q2  | 2.9   |\n| 2024Q1  | 3.0   |\n**AxisTitles**\n**AxisTitles.xAxisTitle**: \n**AxisTitles.yAxisTitle**: \n**FootnotesAndAnnotations**: Source: Federal Reserve.\n\n"
        }
      ],
      "execution_count": 63,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment the first line below to load in a previously cached result.\n",
        "# output = open(\"sample_report.cache\").read()\n",
        "document_content = json.loads(output)\n",
        "document_content = document_content['analyzeResult']['content']"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1744303366471
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(document_content)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "str"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1744303373005
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nicely print document content\n",
        "# print(document_content)"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1744303374831
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enhanced RAG"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a simple (and not exhaustive) starting point. Feel free to give your own chunking strategies a try!\n",
        "\n",
        "In the following example:\n",
        "- we use semantic chunking to chunk the output from document intelligence (enriched with content understading on charts)\n",
        "- we use an embedding model to embed the chunks and store them in Azure AI Search for retrieval\n",
        "- we retrieve the most relevant chunks based on a sample query (e.g. \"Which is the country with the lowest GDP in 2023?\")\n",
        "- we inject the question and the retrieved content in the LLM prompt to give the LLM a context\n",
        "- we inspect the LLM response"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunk text by splitting with Markdown header splitting and recursive character splitting"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Configure langchain text splitting settings\n",
        "EMBEDDING_CHUNK_SIZE = 512\n",
        "EMBEDDING_CHUNK_OVERLAP = 20\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\")\n",
        "]\n",
        "\n",
        "# First split text using Markdown headers\n",
        "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
        "chunks = text_splitter.split_text(document_content)\n",
        "print(\"Number of chunks after semantic chunking: \" + str(len(chunks)))\n",
        "\n",
        "# Then further split the text using recursive character text splitting\n",
        "# It first attempts to split the text using the highest-priority separator (\"<!--\"). \n",
        "# If the resulting chunks are still too large (i.e., exceed chunk_size), \n",
        "# it recursively applies the next separator (\"\\n\\n\"), and so on.\n",
        "char_text_splitter = RecursiveCharacterTextSplitter(separators=[\"<!--\", \"\\n\\n\", \"#\"], chunk_size=EMBEDDING_CHUNK_SIZE, chunk_overlap=EMBEDDING_CHUNK_OVERLAP, is_separator_regex=True)\n",
        "chunks = char_text_splitter.split_documents(chunks)\n",
        "\n",
        "print(\"Final number of chunks: \" + str(len(chunks)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of chunks after semantic chunking: 6\nFinal number of chunks: 19\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1744303378605
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(chunks)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "list"
          },
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1744303381122
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(chunks[0])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": "langchain_core.documents.base.Document"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1744303381805
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [AI Foundry SDK] Calculate embeddings and populate the Azure AI Search index"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# These packages are required for Azure AI Foundry SDK (see requirements_aisearch.txt for packages versions)\n",
        "# %pip install azure-ai-projects\n",
        "# %pip install azure-ai-inference\n",
        "\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1744303383656
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These dependencies handles the connection to Azure Search and the processing of the documents into the index\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.ai.projects.models import ConnectionType\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.search.documents.indexes import SearchIndexClient"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1744303384553
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get your AI Foundry project connection string from the AI Foundry portal\n",
        "project_connection_string=\"swedencentral.api.azureml.ms;dbc342d5-96b5-4aef-a49d-5f6cbd7db6ce;aifoundry-upskilling-rg;aifoundry-upskilling-pj\"\n",
        "\n",
        "# Initialize the AI Foundry project client\n",
        "project = AIProjectClient.from_connection_string(\n",
        "  conn_str=project_connection_string,\n",
        "  credential=DefaultAzureCredential())"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1744303385429
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vector embeddings client that will be used to generate vector embeddings\n",
        "# (at least one AI model that supports text embeddings must be deployed in the project)\n",
        "embeddings = project.inference.get_embeddings_client()\n",
        "\n",
        "type(embeddings)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 28,
          "data": {
            "text/plain": "azure.ai.inference._patch.EmbeddingsClient"
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1744303389427
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Azure AI Search resource\n",
        "print(f\"Search endpoint: {AZURE_SEARCH_ENDPOINT}\")\n",
        "\n",
        "# Azure AI Search index name\n",
        "# load_dotenv(dotenv_path='../infra/credentials.env', override=True)\n",
        "# AZURE_SEARCH_INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") or \"sample-index-visual-doc\"\n",
        "# AZURE_SEARCH_INDEX_NAME = f\"{AZURE_SEARCH_INDEX_NAME}-{datetime.now().strftime('%Y-%m-%d')}-sdk\"\n",
        "AZURE_SEARCH_INDEX_NAME = \"sample-index-visual-doc\"\n",
        "print(f\"Saving on index: {AZURE_SEARCH_INDEX_NAME}\") #-{datetime.now().strftime('%Y-%m-%d')}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Search endpoint: https://ai-search-abutneva687267079310.search.windows.net\nSaving on index: sample-index-visual-doc\n"
        }
      ],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1744303438465
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the project client to get the default search connection\n",
        "# Ensure that you have an Azure AI Search among the connected resources for your AI Foundry project\n",
        "search_connection = project.connections.get_default(\n",
        "    connection_type=ConnectionType.AZURE_AI_SEARCH,\n",
        "    include_credentials=True)\n",
        "\n",
        "# Print to check it\n",
        "# search_connection"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1744303443701
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a client to interact with Azure search service index\n",
        "index_client = SearchIndexClient(\n",
        "    endpoint=search_connection.endpoint_url,\n",
        "    credential=AzureKeyCredential(key=search_connection.key)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1744303445437
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.search.documents.indexes.models import (\n",
        "    SemanticSearch,\n",
        "    SearchField,\n",
        "    SimpleField,\n",
        "    SearchableField,\n",
        "    SearchFieldDataType,\n",
        "    SemanticConfiguration,\n",
        "    SemanticPrioritizedFields,\n",
        "    SemanticField,\n",
        "    VectorSearch,\n",
        "    HnswAlgorithmConfiguration,\n",
        "    VectorSearchAlgorithmKind,\n",
        "    HnswParameters,\n",
        "    VectorSearchAlgorithmMetric,\n",
        "    ExhaustiveKnnAlgorithmConfiguration,\n",
        "    ExhaustiveKnnParameters,\n",
        "    VectorSearchProfile,\n",
        "    SearchIndex\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1744303446806
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_index_definition(index_name: str, model: str) -> SearchIndex:\n",
        "    dimensions = 1536  # text-embedding-ada-002\n",
        "    if model == \"text-embedding-3-large\":\n",
        "        dimensions = 3072\n",
        "\n",
        "    # The fields we want to index. The \"embedding\" field is a vector field that will\n",
        "    # be used for vector search.\n",
        "    fields = [\n",
        "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
        "        SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
        "        SimpleField(name=\"filepath\", type=SearchFieldDataType.String),\n",
        "        SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
        "        SimpleField(name=\"url\", type=SearchFieldDataType.String),\n",
        "        SearchField(\n",
        "            name=\"contentVector\",\n",
        "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
        "            searchable=True,\n",
        "            # Size of the vector created by the embedding model\n",
        "            vector_search_dimensions=dimensions,\n",
        "            vector_search_profile_name=\"myHnswProfile\",\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    # The \"content\" field should be prioritized for semantic ranking.\n",
        "    semantic_config = SemanticConfiguration(\n",
        "        name=\"default\",\n",
        "        prioritized_fields=SemanticPrioritizedFields(\n",
        "            title_field=SemanticField(field_name=\"title\"),\n",
        "            keywords_fields=[],\n",
        "            content_fields=[SemanticField(field_name=\"content\")],\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # For vector search, we want to use the HNSW (Hierarchical Navigable Small World)\n",
        "    # algorithm (a type of approximate nearest neighbor search algorithm) with cosine\n",
        "    # distance.\n",
        "    vector_search = VectorSearch(\n",
        "        algorithms=[\n",
        "            HnswAlgorithmConfiguration(\n",
        "                name=\"myHnsw\",\n",
        "                kind=VectorSearchAlgorithmKind.HNSW,\n",
        "                parameters=HnswParameters(\n",
        "                    m=4,\n",
        "                    ef_construction=1000,\n",
        "                    ef_search=1000,\n",
        "                    metric=VectorSearchAlgorithmMetric.COSINE,\n",
        "                ),\n",
        "            ),\n",
        "            ExhaustiveKnnAlgorithmConfiguration(\n",
        "                name=\"myExhaustiveKnn\",\n",
        "                kind=VectorSearchAlgorithmKind.EXHAUSTIVE_KNN,\n",
        "                parameters=ExhaustiveKnnParameters(metric=VectorSearchAlgorithmMetric.COSINE),\n",
        "            ),\n",
        "        ],\n",
        "        profiles=[\n",
        "            VectorSearchProfile(\n",
        "                name=\"myHnswProfile\",\n",
        "                algorithm_configuration_name=\"myHnsw\",\n",
        "            ),\n",
        "            VectorSearchProfile(\n",
        "                name=\"myExhaustiveKnnProfile\",\n",
        "                algorithm_configuration_name=\"myExhaustiveKnn\",\n",
        "            ),\n",
        "        ],       \n",
        "    )\n",
        "\n",
        "    # Create the semantic settings with the configuration\n",
        "    semantic_search = SemanticSearch(configurations=[semantic_config])\n",
        "\n",
        "    # Create the search index definition\n",
        "    return SearchIndex(\n",
        "        name=index_name,\n",
        "        fields=fields,\n",
        "        semantic_search=semantic_search,\n",
        "        vector_search=vector_search,\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1744303448481
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(AZURE_SEARCH_INDEX_NAME)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "sample-index-visual-doc\n"
        }
      ],
      "execution_count": 34,
      "metadata": {
        "gather": {
          "logged": 1744303450866
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_definition = create_index_definition(AZURE_SEARCH_INDEX_NAME, model=\"text-embedding-ada-002\")\n",
        "index_client.create_index(index_definition)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 35,
          "data": {
            "text/plain": "<azure.search.documents.indexes.models._index.SearchIndex at 0x7fd6a58c61d0>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 35,
      "metadata": {
        "gather": {
          "logged": 1744303481790
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://learn.microsoft.com/en-us/azure/search/tutorial-rag-build-solution-models#configure-search-engine-access-to-azure-models\n",
        "# Assign Cognitive Services OpenAI User + Azure AI Developer role to the current user and to the Azure AI Search system-managed identity.\n",
        "\n",
        "# Resource was added as Search Index Data Contributor at subscription level.\n",
        "# Resource was added as Search Service Contributor at subscription level.\n",
        "\n",
        "# Create a client to interact with an existing Azure Search index\n",
        "search_client = SearchClient(\n",
        "\tindex_name=AZURE_SEARCH_INDEX_NAME,\n",
        "\tendpoint=AZURE_SEARCH_ENDPOINT,\n",
        "\tcredential=AzureKeyCredential(search_connection.key)  # Use the correct key from the search_connection\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 36,
      "metadata": {
        "gather": {
          "logged": 1744303483496
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to process the current list of chunks,\n",
        "# generate vector embeddings, and prepare them for indexing.\n",
        "def process_chunks(chunks: list[dict], model: str) -> list[dict]:\n",
        "    items = []\n",
        "    for chunk in chunks:\n",
        "        content = chunk[\"content\"]\n",
        "        id = str(chunk[\"id\"])\n",
        "        title = chunk.get(\"title\", \"\")\n",
        "        url = chunk.get(\"url\", f\"/documents/{id}\")\n",
        "        emb = embeddings.embed(input=content, model=model)\n",
        "        rec = {\n",
        "            \"id\": id,\n",
        "            \"content\": content,\n",
        "            # \"filepath\": chunk.get(\"filepath\", \"\"),\n",
        "            \"title\": title,\n",
        "            \"url\": url,\n",
        "            \"contentVector\": emb.data[0].embedding,\n",
        "        }\n",
        "        items.append(rec)\n",
        "\n",
        "    return items"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "gather": {
          "logged": 1744303484825
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert chunks to the expected format (list of dictionaries)\n",
        "formatted_chunks = [\n",
        "\t{\n",
        "\t\t\"content\": chunk.page_content,  # Extract the content\n",
        "\t\t\"id\": chunk.metadata.get(\"id\", str(index)),  # Use metadata 'id' or fallback to index\n",
        "\t\t\"title\": chunk.metadata.get(\"title\", \"\"),  # Extract title if available\n",
        "\t\t\"url\": chunk.metadata.get(\"url\", \"\"),  # Extract URL if available\n",
        "\t\t# \"filepath\": chunk.metadata.get(\"filepath\", \"\"),  # Extract filepath if available\n",
        "\t}\n",
        "\tfor index, chunk in enumerate(chunks)\n",
        "]"
      ],
      "outputs": [],
      "execution_count": 38,
      "metadata": {
        "gather": {
          "logged": 1744303504464
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[0]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 39,
          "data": {
            "text/plain": "Document(metadata={}, page_content='<!-- PageHeader=\"UNITED STATES\" -->')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 39,
      "metadata": {
        "gather": {
          "logged": 1744303506509
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_chunks[0]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 40,
          "data": {
            "text/plain": "{'content': '<!-- PageHeader=\"UNITED STATES\" -->',\n 'id': '0',\n 'title': '',\n 'url': ''}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 40,
      "metadata": {
        "gather": {
          "logged": 1744303508463
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload documents to the search index\n",
        "search_client.upload_documents(process_chunks(formatted_chunks, model=\"text-embedding-ada-002\"))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 41,
          "data": {
            "text/plain": "[<azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09a94e0>,\n <azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09a9480>,\n <azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09ab610>,\n <azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09a95a0>,\n <azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09a9750>,\n <azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09a92d0>,\n <azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09aada0>,\n <azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09aaaa0>,\n <azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09aa410>,\n <azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09aa3b0>,\n <azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09aa290>,\n <azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09a9cf0>,\n <azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09a9d50>,\n <azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09a9db0>,\n <azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09a9e10>,\n <azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09a9e70>,\n <azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09a9ed0>,\n <azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09a9f30>,\n <azure.search.documents._generated.models._models_py3.IndexingResult at 0x7fd6c09aa710>]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 41,
      "metadata": {
        "gather": {
          "logged": 1744303510734
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of documents in the index \n",
        "search_client.get_document_count()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 42,
          "data": {
            "text/plain": "19"
          },
          "metadata": {}
        }
      ],
      "execution_count": 42,
      "metadata": {
        "gather": {
          "logged": 1744303511495
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [AI Foundry SDK] Query vector index to retrieve relevant documents"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.inference.prompts import PromptTemplate\n",
        "from azure.search.documents.models import VectorizedQuery"
      ],
      "outputs": [],
      "execution_count": 43,
      "metadata": {
        "gather": {
          "logged": 1744303514467
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chat completion client\n",
        "chat = project.inference.get_chat_completions_client()"
      ],
      "outputs": [],
      "execution_count": 44,
      "metadata": {
        "gather": {
          "logged": 1744303516919
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"What was the crude oil production in 2019?\"\n",
        "query = \"What was the country with the lowest real GDP in 2023?\""
      ],
      "outputs": [],
      "execution_count": 45,
      "metadata": {
        "gather": {
          "logged": 1744303517170
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_product_documents(search_query: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Retrieves the top 5 documents from the Azure AI Search index that are most relevant to the given search query.\n",
        "\n",
        "    Args:\n",
        "        search_query (str): The search query string to find relevant documents.\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: A list of dictionaries, where each dictionary represents a document with the following keys:\n",
        "            - \"id\" (str): The unique identifier of the document.\n",
        "            - \"content\" (str): The content of the document.\n",
        "            - \"title\" (str): The title of the document.\n",
        "            - \"url\" (str): The URL of the document.\n",
        "    \"\"\"\n",
        "    # generate a vector representation of the search query\n",
        "    embedding = embeddings.embed(model=\"text-embedding-ada-002\", input=search_query)\n",
        "    search_vector = embedding.data[0].embedding\n",
        "\n",
        "    # search the index for products matching the search query\n",
        "    vector_query = VectorizedQuery(vector=search_vector, k_nearest_neighbors=5, fields=\"contentVector\")\n",
        "\n",
        "    search_results = search_client.search(\n",
        "        search_text=search_query, vector_queries=[vector_query], select=[\"id\", \"content\", \"title\", \"url\"] # [\"id\", \"content\", \"filepath\", \"title\", \"url\"]\n",
        "    )\n",
        "\n",
        "    documents = [\n",
        "        {\n",
        "            \"id\": result[\"id\"],\n",
        "            \"content\": result[\"content\"],\n",
        "            # \"filepath\": result[\"filepath\"],\n",
        "            \"title\": result[\"title\"],\n",
        "            \"url\": result[\"url\"],\n",
        "        }\n",
        "        for result in search_results\n",
        "    ]\n",
        "\n",
        "    print(f\"📄 {len(documents)} documents retrieved\") #: {documents}\")\n",
        "    return documents"
      ],
      "outputs": [],
      "execution_count": 46,
      "metadata": {
        "gather": {
          "logged": 1744303518458
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_documents = get_product_documents(query)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "📄 11 documents retrieved\n"
        }
      ],
      "execution_count": 47,
      "metadata": {
        "gather": {
          "logged": 1744303519955
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to merge the metadata and content of the retrieved documents into a single context string\n",
        "def generate_context(chunks):\n",
        "    context = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        s = (f\"Source {i} Metadata: {chunk['id']}\\n\"\n",
        "             f\"Source {i} Content: {chunk['content']}\")\n",
        "        context.append(s)\n",
        "    context = '\\n---\\n'.join(context)\n",
        "    return context\n",
        "\n",
        "# Remove redundant chunks\n",
        "appeared = set()\n",
        "unique_chunks = []\n",
        "for chunk in retrieved_documents:\n",
        "    chunk_id = chunk['id']\n",
        "    if chunk_id not in appeared:\n",
        "        appeared.add(chunk_id)\n",
        "        unique_chunks.append(chunk)\n",
        "context = generate_context(unique_chunks)"
      ],
      "outputs": [],
      "execution_count": 48,
      "metadata": {
        "gather": {
          "logged": 1744303522818
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [AI Foundry SDK] Generate answer to query"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define system prompt template for chat model\n",
        "GROUNDED_PROMPT = \"\"\"\n",
        "You are an expert in document analysis. You are proficient in reading and analyzing technical reports. You are good at numerical reasoning and have a good understanding of financial concepts. You are given a question which you need to answer based on the references provided. To answer this question, you may first read the question carefully to know what information is required or helpful to answer the question. Then, you may read the references to find the relevant information.\n",
        "\n",
        "If you find enough information to answer the question, you can first write down your thinking process and then provide a concise answer at the end.\n",
        "If you find that there is not enough information to answer the question, you can state that there is insufficient information.\n",
        "If you are not able or sure how to answer the question, say that you are not able to answer the question.\n",
        "Do not provide any information that is not present in the references.\n",
        "References are in markdown format, you may follow the markdown syntax to better understand the references.\n",
        "\n",
        "---\n",
        "References:\n",
        "{{context}}\n",
        "---\n",
        "\n",
        "Now, here is the question:\n",
        "---\n",
        "Question:\n",
        "{{question}}\n",
        "---\n",
        "Thinking Process::: \n",
        "Answer::: \n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": 49,
      "metadata": {
        "gather": {
          "logged": 1744303524997
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grounded_chat_prompt = PromptTemplate.from_string(GROUNDED_PROMPT)"
      ],
      "outputs": [],
      "execution_count": 50,
      "metadata": {
        "gather": {
          "logged": 1744303527470
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grounded_chat_prompt.create_messages(context = context, question = query)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 51,
          "data": {
            "text/plain": "[{'role': 'system',\n  'content': \"You are an expert in document analysis. You are proficient in reading and analyzing technical reports. You are good at numerical reasoning and have a good understanding of financial concepts. You are given a question which you need to answer based on the references provided. To answer this question, you may first read the question carefully to know what information is required or helpful to answer the question. Then, you may read the references to find the relevant information.\\n\\nIf you find enough information to answer the question, you can first write down your thinking process and then provide a concise answer at the end.\\nIf you find that there is not enough information to answer the question, you can state that there is insufficient information.\\nIf you are not able or sure how to answer the question, say that you are not able to answer the question.\\nDo not provide any information that is not present in the references.\\nReferences are in markdown format, you may follow the markdown syntax to better understand the references.\\n\\n---\\nReferences:\\nSource 0 Metadata: 2\\nSource 0 Content: &lt;!-- FigureContent=&quot;**Title**: 2023 Real GDP\\n**ChartType**: bar\\n**TopicKeywords**: Business and finance, Economics\\n**DetailedDescription**: The bar chart titled '2023 Real GDP' shows the percent deviation from the pre-crisis trend for various regions and countries. The United States is the only region with a positive deviation, slightly above 0.5%. Japan, Canada, and the Euro Area have negative deviations, all around -1.0%. The United Kingdom has the largest negative deviation, close to -4.0%. The G-20 Emerging Markets (EMs) also show a significant negative deviation, slightly above -3.0%.\\n**Summary**: The chart illustrates the 2023 Real GDP percent deviation from the pre-crisis trend for several regions. The US shows a positive deviation, while Japan, Canada, Euro Area, UK, and G-20 EMs show negative deviations, with the UK having the largest negative deviation.\\n**MarkdownDataTable**: | Region     | Percent Deviation |\\n|------------|------------------|\\n| US         | 0.5              |\\n| Japan      | -1.0             |\\n| Canada     | -1.0             |\\n| Euro Area  | -1.0             |\\n| UK         | -4.0             |\\n| G-20 EMs   | -3.0             |\\n**AxisTitles**\\n**AxisTitles.xAxisTitle**:\\n**AxisTitles.yAxisTitle**: Percent Deviation\\n**FootnotesAndAnnotations**: (percent deviation from pre-crisis trend)\\n&quot; --&gt;&lt;figure&gt;  \\nG-20 EMs  \\n&lt;/figure&gt;  \\nSources: IMF World Economic Outlook.\\nNote: Bars show the difference in real output in 2023 and anticipated output for the same\\nperiod prior to the crisis (January 2020 WEO).\\n---\\nSource 1 Metadata: 4\\nSource 1 Content: # A. What Has Underpinned the Post-Pandemic Strength in Demand?  \\n2\\\\. The unprecedented increase in the fiscal deficit during the depths of the pandemic\\nprovided significant fuel to demand. The cumulative increase in federal spending in 2020-21 was\\naround 19 percent of GDP. While somewhat smaller than that of the Euro Area, the extraordinary\\nbreadth of this (relatively untargeted) fiscal support is providing a material boost to aggregate\\ndemand in 2023-24 through multiple channels:  \\n· Federal transfers to households (through stimulus checks, food assistance, child and earned\\nincome tax credits, and unemployment insurance), combined with foregone consumption and\\ndebt rescheduling (e.g., of mortgages and student loans), added an estimated 10 percent of\\nGDP to household savings by end-2021. These resources were then subsequently available to\\nsupport consumption even as real disposable income fell (due to the post-pandemic burst of\\ninflation).  \\n· Large transfers to state and local governments prevented a drawdown of rainy-day funds in the\\npandemic, providing subnational governments with sizable buffers which, in turn, allowed them\\nto maintain their spending above pre-pandemic levels.  \\n· Around 31/2 percent of GDP in federal loans provided through the Payroll Protection Program\\nwere subsequently forgiven, bolstering corporate balance sheets. Other targeted pandemic\\nmeasures (e.g., for airlines) also supported the corporate sector.  \\n\\n---\\nSource 2 Metadata: 1\\nSource 2 Content: # A RESILIENT ECONOMY  \\n1\\\\. The U.S. economy has turned in a remarkable performance over the past few years.  \\nRather than facing lasting negative hysteresis effects from the pandemic, the U.S. is the only G20\\neconomy that is now operating above the levels of output and employment expected prior to the\\npandemic. Q4/Q4 growth in 2023 (at 3.1 percent)\\nwas almost three times that expected at the time\\n2023 Real GDP\\n(percent deviation from pre-crisis trend)\\nof the last Article IV and core PCE inflation was\\n1.0\\nalmost 1 percentage point lower. The rebound has\\nbeen characterized by important gains on both the\\n-0.5\\ndemand and supply side which has allowed\\n-2.0\\ninflation to head back to the FOMC's medium-term\\n-3.5\\ntarget without a major dislocation in the real\\neconomy. The strength of the U.S. economy and\\n-5.0\\nthe relatively quick disinflation have had large,\\nUS\\nJapan\\nCanada\\nEuro Area\\nUK\\npositive spillovers to the world economy.  \\n\\n---\\nSource 3 Metadata: 7\\nSource 3 Content: &lt;!-- FigureContent=&quot;**Title**: Federal Government Spending\\n**ChartType**: bar\\n**TopicKeywords**: Business and finance, Economics, Government\\n**DetailedDescription**: The chart displays federal government spending as a percentage of GDP, relative to the 2019 level, from 2020 to 2023. The spending is categorized into four groups: Individuals, Businesses, SLGs (State and Local Governments), and Others. In 2020, the total spending was approximately 9% of GDP, with Individuals accounting for the largest share, followed by Others, Businesses, and SLGs. In 2021, the spending decreased to around 5% of GDP, with Individuals still being the largest category, but with reduced contributions from Others and Businesses. By 2022, the spending further decreased to about 1% of GDP, with Individuals and Others contributing equally. In 2023, the spending slightly increased to about 2% of GDP, with Individuals again being the largest category, followed by Others, Businesses, and SLGs.\\n**Summary**: The chart illustrates the trend of federal government spending from 2020 to 2023, showing a significant decrease from 9% of GDP in 2020 to about 2% in 2023. The spending is broken down into categories, with Individuals consistently being the largest contributor throughout the years.\\n**MarkdownDataTable**: | Year | Individuals | Businesses | SLGs | Others |\\n|------|-------------|------------|------|--------|\\n| 2020 | 6           | 1          | 0.5  | 1.5    |\\n| 2021 | 3.5         | 0.5        | 0.5  | 0.5    |\\n| 2022 | 0.5         | 0.2        | 0.1  | 0.2    |\\n| 2023 | 1           | 0.3        | 0.2  | 0.5    |\\n**AxisTitles**\\n**AxisTitles.xAxisTitle**: Year\\n**AxisTitles.yAxisTitle**: Percent of GDP\\n**FootnotesAndAnnotations**: Source: U.S. Department of Treasury; Small Business Administration; Congressional Research Services; Office of Management and Budget; IMF staff calculation.\\nNote: 'Federal spending is measured as the increase relative to the 2019 level; Others included defense spending, capital investment, interest spending, and other items not directly attributable to firms, households, or subnational governments.\\n&quot; --&gt;&lt;figure&gt;\\n&lt;figcaption&gt;General Government Structural Primary Balance\\n(in percent of GDP)&lt;/figcaption&gt;  \\nFederal Government Spending1\\n(percent of GDP; relative to the 2019 level)  \\n10  \\nIndividuals  \\n8  \\nBusinesses  \\n6  \\nSLGs  \\nOthers  \\n4  \\n2  \\n0  \\n-2  \\n2020  \\n2021  \\n2022  \\n2023  \\nSource: U.S. Department of Tresury; Small Business Administration; Congressional\\nResearch Services; Office of Management and Budget; IMF staff calculation.\\nNote: 1Federal spending is measured as the increase relative to the 2019 level; Others included\\ndefense spending, capital investment, interest spending, and other items not directly attributable\\nto firms, households, or subnational governments.  \\n&lt;/figure&gt;  \\n\\n---\\nSource 4 Metadata: 14\\nSource 4 Content: &lt;!-- PageHeader=&quot;UNITED STATES&quot; --&gt;  \\n4\\\\.\\nThis strength in aggregate consumption, however, masks an upswing in poverty and\\nrising signs of economic distress among low-income households. During the recovery from the\\npandemic, labor shortages lifted real wages at the bottom of the income distribution and led to a\\ncompression of wage inequality.2 Despite these gains, poverty increased by 4.6 percentage points in\\n2022 and the child poverty rate more than doubled.3 This rise in poverty can be directly attributed to\\nthe expiration of pandemic-era assistance, particularly changes that had been made to the Child Tax\\nCredit and the Earned Income Tax Credit (EITC).\\nDeliquency Rate on Credit Card Loans - All Commercial Banks\\n(percent, seasonally Adjusted)\\nThe increased pressure on lower income\\nhouseholds is becoming more visible in an\\n3\\nupswing in delinquencies on revolving credit.\\nFurthermore, worsening housing affordability has\\n2.5\\naggravated access to shelter, particularly for the\\nyoung and lower income households. This is\\n2\\nevident in the number of people experiencing\\nhomelessness, which has risen to the highest level\\nsince data began to be compiled in 2007.4\\n1.5\\n2018Q1 2018Q4 2019Q3 2020Q2 2021Q1 2021Q4 2022Q3 2023Q2 2024Q1  \\n\\n---\\nSource 5 Metadata: 11\\nSource 5 Content: &lt;!-- FigureContent=&quot;**Title**: Asset Price Indices (Dec 2019 = 100)\\n**ChartType**: line\\n**TopicKeywords**: Business and finance, Economics, Real estate, Stock market\\n**DetailedDescription**: The chart displays two asset price indices from December 2019 to September 2023, with December 2019 set as the base value of 100. The Case-Shiller House Price Index is represented by a blue line, while the S&amp;P 500 Index is shown in orange. The Case-Shiller House Price Index shows a steady increase over the period, starting at 100 in December 2019 and reaching approximately 150 by September 2023. The S&amp;P 500 Index exhibits more volatility, initially dropping below 100 in early 2020, then rising sharply to peak around 160 in late 2021, followed by fluctuations and ending at approximately 160 in September 2023.\\n**Summary**: The Case-Shiller House Price Index has shown a consistent upward trend from December 2019 to September 2023, indicating a steady increase in house prices. In contrast, the S&amp;P 500 Index has experienced significant volatility, with a sharp rise and subsequent fluctuations, ultimately reaching a similar level to the Case-Shiller Index by September 2023.\\n**MarkdownDataTable**: | Date     | Case-Shiller House Price Index | S&amp;P 500 Index |\\n|----------|-------------------------------|---------------|\\n| Dec-2019 | 100                           | 100           |\\n| Sep-2020 | 110                           | 120           |\\n| Jun-2021 | 130                           | 150           |\\n| Mar-2022 | 140                           | 160           |\\n| Dec-2022 | 145                           | 150           |\\n| Sep-2023 | 150                           | 160           |\\n**AxisTitles**\\n**AxisTitles.xAxisTitle**:\\n**AxisTitles.yAxisTitle**: Index Value\\n**FootnotesAndAnnotations**: Source: S&amp;P Dow Jones Indices LLC; Haver.\\n&quot; --&gt;&lt;figure&gt;\\n&lt;figcaption&gt;Net Worth across Income Quintiles\\n(US$ trillions, constant 2019 prices)&lt;/figcaption&gt;  \\nAsset Price Indices\\n(Dec 2019 = 100)  \\n170  \\n160  \\n150  \\n140  \\n130  \\n120  \\n110  \\n100  \\n-Case-Shiller House Price Index  \\n90  \\n-S&amp;P 500 Index  \\n80  \\nDec-2019 Sep-2020 Jun-2021 Mar-2022 Dec-2022 Sep-2023\\nSource: S&amp;P Dow Jones Indices LLC; Haver.  \\n&lt;/figure&gt;  \\n\\n---\\nSource 6 Metadata: 12\\nSource 6 Content: &lt;!-- FigureContent=&quot;**Title**: (US$ trillions, constant 2019 prices)\\n**ChartType**: bar\\n**TopicKeywords**: Business and finance, Economics, Income distribution\\n**DetailedDescription**: The bar chart displays the distribution of income across different segments of the population, divided into quintiles, for the years 2009, 2019, and 2023. The income is measured in US$ trillions, adjusted to constant 2019 prices. The segments are Bottom 20%, 21-40%, 41-60%, 61-80%, and Top 20% (RHS). The Top 20% segment is shown on a secondary axis (RHS) with a different scale. In 2009, the Bottom 20% had the lowest income, while the Top 20% had the highest. Over the years, income has increased across all segments, with the Top 20% showing the most significant growth. By 2023, the Top 20% segment reached nearly 90 trillion, while the Bottom 20% remained below 2 trillion.\\n**Summary**: The chart illustrates the growth in income across different population segments from 2009 to 2023, with the Top 20% showing the most substantial increase. The income is measured in constant 2019 US$ trillions, highlighting economic disparities.\\n**MarkdownDataTable**: | Segment   | 2009 | 2019 | 2023 |\\n|-----------|------|------|------|\\n| Bottom 20%| 1    | 2    | 2    |\\n| 21-40%    | 2    | 3    | 3    |\\n| 41-60%    | 3    | 4    | 5    |\\n| 61-80%    | 5    | 7    | 8    |\\n| Top 20%   | 60   | 80   | 90   |\\n**AxisTitles**\\n**AxisTitles.xAxisTitle**:\\n**AxisTitles.yAxisTitle**: US$ trillions\\n**FootnotesAndAnnotations**: Source: Federal Reserve; Bureau of Economic Analysis\\n&quot; --&gt;&lt;figure&gt;  \\n18  \\n100  \\n16  \\n2009 2019 2023  \\n90  \\n14  \\n80  \\n12  \\n70  \\n60  \\n10  \\n50  \\n8  \\n40  \\n6  \\n30  \\n4  \\n20  \\n2  \\n10  \\nBottom 20%  \\n21-40%  \\n41-60%  \\n61-80%  \\nTop 20%\\n(RHS)  \\nSource: Federal Reserve; Bureau of Economic Analysis.  \\n&lt;/figure&gt;  \\n1 Data on net wealth is drawn from the Survey of Consumer Finances and Financial Accounts of the United States\\nproduced by the Federal Reserve Board, and the Household Finance and Consumption Survey and Quarterly Sector\\nAccounts produced by the European System of Central Banks. The change in real median net wealth is calculated\\nfrom 2019Q3 to 2022Q3.  \\n\\n---\\nSource 7 Metadata: 17\\nSource 7 Content: # 5. Both households and corporates have been insulated from the impact of higher interest rates.  \\n. During the pandemic, both corporate and household borrowers locked in low rates at long\\nmaturities, paying down higher cost, floating rate debt. Around one half of mortgages reset at\\nlower rates during 2020-21 (either through refinancing operations or when the mortgage\\nrefreshed through home sales). As a result, by end-2021, over 95 percent of mortgages were at\\nlow, fixed rates.5 Similarly, the average duration of investment grade corporate bonds rose\\nduring 2020-21 with new debt being contracted at relatively low yields.  \\n· Firms and households were able to increase their holdings of short duration financial assets.\\nHouseholds extracted around US$0.5 trillion in home equity during 2020-21 which, alongside\\nthe higher savings described above, increased their holdings of bank deposits and money\\nmarket funds by almost 3 percent of GDP. Similarly, nonfinancial corporates now hold\\n2.3 percent of GDP more in bank deposits and money market funds than they did at end-2019.  \\nTogether, these two dynamics have resulted in a muted impact of higher interest rates on\\nconsumption and corporate investment.6 Indeed, over the past two years, the combination of low-  \\n2 See, for example, Autor, Dube and McGrew (2023).  \\n3 As measured by the supplementary poverty measure that accounts for income and payroll taxes, tax credits, and\\nnon-cash transfers while using geographically adjusted poverty thresholds (Census Bureau, 2023).  \\n4 See Annual Homelessness Assessment Report, December 2023.  \\n5 See A. Haughwout et al., &quot;The Great Pandemic Mortgage Refinance Boom&quot;, Federal Reserve Bank of New York.  \\n6 Higher interest rates did though have a clear impact through other channels, notably subtracting around 1/2 percent\\nfrom growth in both 2022 and 2023 as a result of the decline in residential investment.  \\n\\n---\\nSource 8 Metadata: 10\\nSource 8 Content: # 3. Rising household wealth has been a key determinant in supporting consumer demand  \\n(Box 1). The significant build-up of savings during the pandemic (described above) allowed\\nhouseholds to pay down costly revolving credit, build-up financial assets, and subsequently benefit\\nfrom the post-pandemic run-up in asset prices. In addition, homeowners benefited from an almost\\n50 percent increase in the average house price since end-2019. As a result, real median net wealth in\\nthe U.S. has grown by 34 percent since 2019 (compared to a 5 percent increase in the Euro Area\\nover the same period).1 Notably, wealth rose across the whole income distribution (albeit with much\\nlarger gains for the highest income households).  \\n\\n---\\nSource 9 Metadata: 15\\nSource 9 Content: &lt;!-- FigureContent=&quot;**Title**:\\n**ChartType**: line\\n**TopicKeywords**: Business and finance, Economics\\n**DetailedDescription**: The line chart shows a trend over time from 2018 Q1 to 2024 Q1. The values start at approximately 2.5 in 2018 Q1, showing a slight increase and fluctuation until 2020 Q1. From 2020 Q1, there is a sharp decline reaching a low point of approximately 1.5 in 2021 Q1. After this point, the values begin to rise steadily, surpassing the initial value by 2023 Q3 and continuing to increase sharply, reaching a peak of 3 by 2024 Q1.\\n**Summary**: The chart illustrates a significant dip in values around 2020 Q1 to 2021 Q1, followed by a strong recovery and growth, reaching a peak by 2024 Q1. This trend may indicate economic recovery or growth after a downturn.\\n**MarkdownDataTable**: | Quarter | Value |\\n|---------|-------|\\n| 2018Q1  | 2.5   |\\n| 2018Q4  | 2.6   |\\n| 2019Q3  | 2.7   |\\n| 2020Q2  | 2.4   |\\n| 2021Q1  | 1.5   |\\n| 2021Q4  | 2.0   |\\n| 2022Q3  | 2.5   |\\n| 2023Q2  | 2.8   |\\n| 2024Q1  | 3.0   |\\n**AxisTitles**\\n**AxisTitles.xAxisTitle**: Quarter\\n**AxisTitles.yAxisTitle**: Value\\n**FootnotesAndAnnotations**: Source: Federal Reserve.\\n&quot; --&gt;&lt;figure&gt;  \\nSource: Federal Reserve.  \\n&lt;/figure&gt;\\n---\\nSource 10 Metadata: 8\\nSource 10 Content: &lt;!-- FigureContent=&quot;**Title**: General Government Structural Primary Balance\\n**ChartType**: bar\\n**TopicKeywords**: Business and finance, Economics, Fiscal policy, Government\\n**DetailedDescription**: The chart compares the general government structural primary balance for the US and the Euro Area in two different periods: pre-pandemic (2019) and during the fiscal impulse period. The structural primary balance is a measure of the government's fiscal position, excluding interest payments on debt. In 2019, the US had a structural primary balance of approximately -3, while the Euro Area had a slightly better balance, close to -2. During the fiscal impulse period, both regions experienced significant increases in cumulative deficits. The US's deficit increased to around -8, while the Euro Area's deficit increased even more dramatically to about -12.\\n**Summary**: The chart illustrates the change in general government structural primary balance from 2019 to the fiscal impulse period for the US and Euro Area. Both regions saw increased deficits, with the Euro Area experiencing a larger fiscal impulse.\\n**MarkdownDataTable**: | Region     | Period          | Structural Primary Balance |\\n|------------|-----------------|----------------------------|\\n| US         | Pre-pandemic    | -3                         |\\n| Euro Area  | Pre-pandemic    | -2                         |\\n| US         | Fiscal impulse  | -8                         |\\n| Euro Area  | Fiscal impulse  | -12                        |\\n**AxisTitles**\\n**AxisTitles.xAxisTitle**: Period\\n**AxisTitles.yAxisTitle**: General Government Structural Primary Balance\\n**FootnotesAndAnnotations**:\\n&quot; --&gt;&lt;figure&gt;  \\n2  \\n-3  \\n-8  \\nUS  \\nEuro Area  \\n-13  \\nPre-pandemic (2019)  \\nFiscal impulse  \\nGeneral Government Sturctural Primary Balance  \\nCumulative deficits in excess of 2019  \\n&lt;/figure&gt;  \\nSource: IMF World Economic Outlook.  \\nNote: Fiscal impulse is calculated as the cumulative sum of general government structural primary\\ndeficits exceeding 2019 levels over the period 2020-2023.\\n---\\n\\nNow, here is the question:\\n---\\nQuestion:\\nWhat was the country with the lowest real GDP in 2023?\\n---\\nThinking Process::: \\nAnswer:::\"}]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 51,
      "metadata": {
        "gather": {
          "logged": 1744303528740
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\"role\": \"user\", \"content\": query}]\n",
        "# documents = get_product_documents(query)\n",
        "# grounded_chat_prompt = PromptTemplate.from_string(GROUNDED_PROMPT)\n",
        "\n",
        "system_message = grounded_chat_prompt.create_messages(context = context, question = query)\n",
        "response = chat.complete(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=system_message + messages,\n",
        "    **grounded_chat_prompt.parameters,\n",
        ")\n",
        "print(f\"💬 Response: {response.choices[0].message}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "💬 Response: {'content': 'Based on the reference from Source 0, which provides a chart detailing the 2023 Real GDP percent deviation from the pre-crisis trend, the United Kingdom had the lowest real GDP in 2023 with a percent deviation close to -4.0%.', 'refusal': None, 'role': 'assistant'}\n"
        }
      ],
      "execution_count": 52,
      "metadata": {
        "gather": {
          "logged": 1744303532392
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(response)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 53,
          "data": {
            "text/plain": "azure.ai.inference.models._patch.ChatCompletions"
          },
          "metadata": {}
        }
      ],
      "execution_count": 53,
      "metadata": {
        "gather": {
          "logged": 1744303534492
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Based on the reference from Source 0, which provides a chart detailing the 2023 Real GDP percent deviation from the pre-crisis trend, the United Kingdom had the lowest real GDP in 2023 with a percent deviation close to -4.0%.\n"
        }
      ],
      "execution_count": 54,
      "metadata": {
        "gather": {
          "logged": 1744303537500
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "my_env",
      "language": "python",
      "display_name": "my_env"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "my_env"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}